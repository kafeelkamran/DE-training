{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62dd7f09-3614-4626-ba0d-1fd3f4720778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"SinkPath\", \"\", \"Enter Sink Path\")\n",
    "sinkPath = dbutils.widgets.get(\"SinkPath\")\n",
    "print(sinkPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94884ceb-de40-481e-b574-78d95b4b30ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json, DataFrame, sha2, concat_ws, lit\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db85ce6-8429-4af7-8978-2dc2b4caea2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\nFile name: Customer/, File path: wasbs://casestudy@k1datastorage.blob.core.windows.net/raw/bronze/Customer/\nFile name: Products/, File path: wasbs://casestudy@k1datastorage.blob.core.windows.net/raw/bronze/Products/\nFile name: Transactions/, File path: wasbs://casestudy@k1datastorage.blob.core.windows.net/raw/bronze/Transactions/\n"
     ]
    }
   ],
   "source": [
    "# Configuration for Azure Blob Storage SAS token\n",
    "storage_account = \"k1datastorage\"\n",
    "container = \"casestudy\"\n",
    "sas_token = \"sp=racwdlmeop&st=2025-08-09T09:56:09Z&se=2025-08-09T18:11:09Z&spr=https&sv=2024-11-04&sr=c&sig=BK5xy4gjRsv7KoX1NqmyOJt5tQQeAAszSDQMg8l3yh4%3D\"\n",
    "\n",
    "# Set SAS token for Azure Blob Storage container\n",
    "spark.conf.set(f\"fs.azure.sas.{container}.{storage_account}.blob.core.windows.net\", sas_token)\n",
    "\n",
    "# Specify the directory or folder path in your container\n",
    "# sinkPath = \"bronzelayer\"  # You can modify this to your actual folder path\n",
    "# List files in the container (without specifying folder path) to check the structure\n",
    "\n",
    "dbutils.fs.ls(f\"wasbs://{container}@{storage_account}.blob.core.windows.net/\")\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "# Delta path to be accessed\n",
    "delta_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/raw/bronze\"\n",
    "\n",
    "# List the files in the specified path\n",
    "files = dbutils.fs.ls(delta_path)\n",
    "\n",
    "# Print out the contents of the path\n",
    "for file in files:\n",
    "    print(f\"File name: {file.name}, File path: {file.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c17505-fcd4-4720-9ce0-5e34e5697233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n\nroot\n |-- _c0: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "cust_path =f\"{delta_path}/Customer\"\n",
    "trans_path =f\"{delta_path}/Transactions\"\n",
    "pro_path =f\"{delta_path}/Products\"\n",
    "\n",
    "cust_df = spark.read.csv(cust_path)\n",
    "trans_df = spark.read.csv(trans_path)\n",
    "pro_df = spark.read.csv(pro_path)\n",
    "cust_df.printSchema()\n",
    "trans_df.printSchema()\n",
    "pro_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51dc240-4975-41eb-b08c-2619332d6ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "no_dup_cust_df = cust_df.dropDuplicates()\n",
    "no_dup_trans_df = trans_df.dropDuplicates()\n",
    "no_dup_pro_df = pro_df.dropDuplicates()\n",
    "\n",
    "silver_path = f\"{delta_path}Silver\"\n",
    "\n",
    "no_dup_cust_df.write.mode(\"overwrite\").format(\"delta\").save(silver_path)\n",
    "no_dup_trans_df.write.mode(\"overwrite\").format(\"delta\").save(silver_path)\n",
    "no_dup_pro_df.write.mode(\"overwrite\").format(\"delta\").save(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e4d524-1c28-44d4-b298-eac0ddf5ce6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|                 _c0|\n+--------------------+\n|                \u0015\u0004\u0015$|\n|\u001C\u0015\u0004\u0015\u0000\u0015\u0006\u0015\\b\u0000\u0000\u0006\u0014\u0002\u0000\u0000...|\n|\u0015\u0016\u0016�\\a\u0015.\u0000&�\u0003\u001C\u0015\\f\u0019...|\n|           <\u0015\u0002\u0015\u0004\u0000\u0000\\f|\n|\u0015\u0016\u0016�\\a\u0015\u001E\u0000&�\u0002\u001C\u0015\u0002\u00195...|\n+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "cust_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "trans_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "pro_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "def mask_pii(df: DataFrame, pii_columns: dict) -> DataFrame:\n",
    "    columns = df.columns\n",
    "\n",
    "    for index, mask_type in pii_columns.items():\n",
    "        if index >= len(columns):\n",
    "            continue  \n",
    "\n",
    "        col_name = columns[index]\n",
    "        if col_name not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if mask_type == \"hash\":\n",
    "            df = df.withColumn(col_name, sha2(col(col_name).cast(\"string\"), 256))\n",
    "        elif mask_type == \"redact\":\n",
    "            df = df.withColumn(col_name, lit(\"REDACTED\"))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported mask type: '{mask_type}' for column '{col_name}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "pii_columns = {\n",
    "    1: \"hash\",   \n",
    "    2: \"hash\",  \n",
    "    3: \"redact\", \n",
    "    4: \"redact\"\n",
    "}\n",
    "\n",
    "masked_customers_df = mask_pii(cust_silver, pii_columns)\n",
    "masked_customers_df.show(5)\n",
    "\n",
    "gold_path = f\"{delta_path}Gold\"\n",
    "\n",
    "masked_customers_df.write.mode(\"overwrite\").format(\"delta\").save(gold_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b857218-4020-4704-baf7-cfc7dc70dae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "k-adf",
   "widgets": {
    "SinkPath": {
     "currentValue": "",
     "nuid": "a33b4766-2d91-4e8b-a74f-1b30306d6f67",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter Sink Path",
      "name": "SinkPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter Sink Path",
      "name": "SinkPath",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}